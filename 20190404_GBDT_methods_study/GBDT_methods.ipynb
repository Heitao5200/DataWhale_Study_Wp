{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度提升树(GBDT)算法\n",
    "\n",
    "Gradient boosting 就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高。之所以称为 Gradient，是因为在添加新模型时使用了梯度下降算法来最小化的损失。\n",
    "利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@author: KM\\n@license: (C) Copyright 2013-2017, Node Supply Chain Manager Corporation Limited.\\n@contact: yangkm601@gmail.com\\n@software: garner\\n@file: GBDT_methods.ipynb\\n@time: 2019/4/3 9:44\\n@desc:  梯度提升树(GBDT)算法\\n@url:  https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/gbdt_classifier.ipynb\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "'''\n",
    "@author: KM\n",
    "@license: (C) Copyright 2013-2017, Node Supply Chain Manager Corporation Limited.\n",
    "@contact: yangkm601@gmail.com\n",
    "@software: garner\n",
    "@file: GBDT_methods.ipynb\n",
    "@time: 2019/4/3 9:44\n",
    "@desc:  梯度提升树(GBDT)算法\n",
    "@url:  https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/gbdt_classifier.ipynb\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先，我们载入需要的类库：　　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "d:\\progrom\\python\\python\\python3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接着，我们把解压的数据用下面的代码载入，顺便看看数据的类别分布。\n",
    "\n",
    "可以看到类别输出如下，也就是类别0的占大多数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19680\n",
       "1      320\n",
       "Name: Disbursed, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_modified/train_modified.csv')\n",
    "target='Disbursed' # Disbursed的值就是二元分类的输出\n",
    "IDcol = 'ID'\n",
    "train['Disbursed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们得到我们的训练集。最后一列Disbursed是分类输出。前面的所有列（不考虑ID列）都是样本特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = [x for x in train.columns if x not in [target, IDcol]]\n",
    "X = train[x_columns]\n",
    "y = train['Disbursed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不管任何参数，都用默认的，我们拟合下数据看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9852\n",
      "AUC Score (Train): 0.900531\n"
     ]
    }
   ],
   "source": [
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "gbm0.fit(X,y)\n",
    "y_pred = gbm0.predict(X)\n",
    "y_predprob = gbm0.predict_proba(X)[:,1]\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 输出如下，可见拟合还可以，我们下面看看怎么通过调参提高模型的泛化能力。\n",
    " \n",
    " \n",
    " 首先我们从步长(learning rate)和迭代次数(n_estimators)入手。一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，我们将步长初始值设置为0.1。对于迭代次数进行网格搜索如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.81285, std: 0.01967, params: {'n_estimators': 20},\n",
       "  mean: 0.81438, std: 0.01947, params: {'n_estimators': 30},\n",
       "  mean: 0.81451, std: 0.01933, params: {'n_estimators': 40},\n",
       "  mean: 0.81618, std: 0.01848, params: {'n_estimators': 50},\n",
       "  mean: 0.81779, std: 0.01736, params: {'n_estimators': 60},\n",
       "  mean: 0.81533, std: 0.01900, params: {'n_estimators': 70},\n",
       "  mean: 0.81322, std: 0.01860, params: {'n_estimators': 80}],\n",
       " {'n_estimators': 60},\n",
       " 0.8177893165650406)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "param_test1 = {'n_estimators':np.arange(20,81,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10),  param_grid = param_test1, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch1.fit(X,y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始代码为：\n",
    "    param_test = {'n_estimators':range(30,151,20)}  \n",
    "报错：\n",
    "    Parameter values for parameter (n_estimators) need to be a sequence.\n",
    "\n",
    "后期修改为：\n",
    "    import numpy as np\n",
    "    param_test1 = {'n_estimators':np.arange(20,81,10)}\n",
    "参考网址：https://stackoverflow.com/questions/45444953/parameter-values-for-parameter-n-estimators-need-to-be-a-sequence\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    找到了一个合适的迭代次数，现在我们开始对决策树进行调参。首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.81199, std: 0.02073, params: {'max_depth': 3, 'min_samples_split': 100},\n",
       "  mean: 0.81267, std: 0.01985, params: {'max_depth': 3, 'min_samples_split': 300},\n",
       "  mean: 0.81238, std: 0.01937, params: {'max_depth': 3, 'min_samples_split': 500},\n",
       "  mean: 0.80925, std: 0.02051, params: {'max_depth': 3, 'min_samples_split': 700},\n",
       "  mean: 0.81846, std: 0.01843, params: {'max_depth': 5, 'min_samples_split': 100},\n",
       "  mean: 0.81630, std: 0.01810, params: {'max_depth': 5, 'min_samples_split': 300},\n",
       "  mean: 0.81315, std: 0.01898, params: {'max_depth': 5, 'min_samples_split': 500},\n",
       "  mean: 0.81262, std: 0.02090, params: {'max_depth': 5, 'min_samples_split': 700},\n",
       "  mean: 0.81807, std: 0.02004, params: {'max_depth': 7, 'min_samples_split': 100},\n",
       "  mean: 0.82137, std: 0.01733, params: {'max_depth': 7, 'min_samples_split': 300},\n",
       "  mean: 0.81703, std: 0.01773, params: {'max_depth': 7, 'min_samples_split': 500},\n",
       "  mean: 0.81383, std: 0.02327, params: {'max_depth': 7, 'min_samples_split': 700},\n",
       "  mean: 0.81107, std: 0.02178, params: {'max_depth': 9, 'min_samples_split': 100},\n",
       "  mean: 0.80944, std: 0.02612, params: {'max_depth': 9, 'min_samples_split': 300},\n",
       "  mean: 0.81476, std: 0.01973, params: {'max_depth': 9, 'min_samples_split': 500},\n",
       "  mean: 0.81601, std: 0.02576, params: {'max_depth': 9, 'min_samples_split': 700},\n",
       "  mean: 0.81101, std: 0.02222, params: {'max_depth': 11, 'min_samples_split': 100},\n",
       "  mean: 0.81309, std: 0.02696, params: {'max_depth': 11, 'min_samples_split': 300},\n",
       "  mean: 0.81713, std: 0.02379, params: {'max_depth': 11, 'min_samples_split': 500},\n",
       "  mean: 0.81347, std: 0.02702, params: {'max_depth': 11, 'min_samples_split': 700},\n",
       "  mean: 0.81484, std: 0.01776, params: {'max_depth': 13, 'min_samples_split': 100},\n",
       "  mean: 0.80825, std: 0.02291, params: {'max_depth': 13, 'min_samples_split': 300},\n",
       "  mean: 0.81923, std: 0.01693, params: {'max_depth': 13, 'min_samples_split': 500},\n",
       "  mean: 0.81382, std: 0.02258, params: {'max_depth': 13, 'min_samples_split': 700}],\n",
       " {'max_depth': 7, 'min_samples_split': 300},\n",
       " 0.8213724275914632)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'max_depth':np.arange(3,14,2), 'min_samples_split':np.arange(100,801,200)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, min_samples_leaf=20, \n",
    "      max_features='sqrt', subsample=0.8, random_state=10), \n",
    "   param_grid = param_test2, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch2.fit(X,y)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
    "# 输出如下，可见最好的最大树深度是7，内部节点再划分所需最小样本数是300。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  由于决策树深度7是一个比较合理的值，我们把它定下来，对于内部节点再划分所需最小样本数min_samples_split，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.81828, std: 0.02251, params: {'min_samples_leaf': 60, 'min_samples_split': 800},\n",
       "  mean: 0.81731, std: 0.02344, params: {'min_samples_leaf': 60, 'min_samples_split': 1000},\n",
       "  mean: 0.82220, std: 0.02250, params: {'min_samples_leaf': 60, 'min_samples_split': 1200},\n",
       "  mean: 0.81447, std: 0.02125, params: {'min_samples_leaf': 60, 'min_samples_split': 1400},\n",
       "  mean: 0.81495, std: 0.01626, params: {'min_samples_leaf': 60, 'min_samples_split': 1600},\n",
       "  mean: 0.81528, std: 0.02140, params: {'min_samples_leaf': 60, 'min_samples_split': 1800},\n",
       "  mean: 0.81590, std: 0.02517, params: {'min_samples_leaf': 70, 'min_samples_split': 800},\n",
       "  mean: 0.81573, std: 0.02207, params: {'min_samples_leaf': 70, 'min_samples_split': 1000},\n",
       "  mean: 0.82021, std: 0.02521, params: {'min_samples_leaf': 70, 'min_samples_split': 1200},\n",
       "  mean: 0.81512, std: 0.01995, params: {'min_samples_leaf': 70, 'min_samples_split': 1400},\n",
       "  mean: 0.81395, std: 0.02081, params: {'min_samples_leaf': 70, 'min_samples_split': 1600},\n",
       "  mean: 0.81587, std: 0.02082, params: {'min_samples_leaf': 70, 'min_samples_split': 1800},\n",
       "  mean: 0.82064, std: 0.02698, params: {'min_samples_leaf': 80, 'min_samples_split': 800},\n",
       "  mean: 0.81490, std: 0.02475, params: {'min_samples_leaf': 80, 'min_samples_split': 1000},\n",
       "  mean: 0.82009, std: 0.02568, params: {'min_samples_leaf': 80, 'min_samples_split': 1200},\n",
       "  mean: 0.81850, std: 0.02226, params: {'min_samples_leaf': 80, 'min_samples_split': 1400},\n",
       "  mean: 0.81855, std: 0.02099, params: {'min_samples_leaf': 80, 'min_samples_split': 1600},\n",
       "  mean: 0.81666, std: 0.02249, params: {'min_samples_leaf': 80, 'min_samples_split': 1800},\n",
       "  mean: 0.81960, std: 0.02437, params: {'min_samples_leaf': 90, 'min_samples_split': 800},\n",
       "  mean: 0.81560, std: 0.02235, params: {'min_samples_leaf': 90, 'min_samples_split': 1000},\n",
       "  mean: 0.81936, std: 0.02542, params: {'min_samples_leaf': 90, 'min_samples_split': 1200},\n",
       "  mean: 0.81362, std: 0.02254, params: {'min_samples_leaf': 90, 'min_samples_split': 1400},\n",
       "  mean: 0.81429, std: 0.02417, params: {'min_samples_leaf': 90, 'min_samples_split': 1600},\n",
       "  mean: 0.81299, std: 0.02262, params: {'min_samples_leaf': 90, 'min_samples_split': 1800},\n",
       "  mean: 0.82000, std: 0.02511, params: {'min_samples_leaf': 100, 'min_samples_split': 800},\n",
       "  mean: 0.82209, std: 0.01816, params: {'min_samples_leaf': 100, 'min_samples_split': 1000},\n",
       "  mean: 0.81821, std: 0.02337, params: {'min_samples_leaf': 100, 'min_samples_split': 1200},\n",
       "  mean: 0.81922, std: 0.02377, params: {'min_samples_leaf': 100, 'min_samples_split': 1400},\n",
       "  mean: 0.81545, std: 0.02221, params: {'min_samples_leaf': 100, 'min_samples_split': 1600},\n",
       "  mean: 0.81704, std: 0.02509, params: {'min_samples_leaf': 100, 'min_samples_split': 1800}],\n",
       " {'min_samples_leaf': 60, 'min_samples_split': 1200},\n",
       " 0.8222032996697154)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {'min_samples_split':np.arange(800,1900,200), 'min_samples_leaf':np.arange(60,101,10)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7,\n",
    "                                     max_features='sqrt', subsample=0.8, random_state=10), \n",
    "                       param_grid = param_test3, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch3.fit(X,y)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_\n",
    "\n",
    "# 输出结果如下，可见这个min_samples_split在边界值，还有进一步调试小于边界60的必要。由于这里只是例子，所以大家可以自己下来用包含小于60的网格搜索来寻找合适的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们调了这么多参数了，终于可以都放到GBDT类里面去看看效果了。现在我们用新参数拟合数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.984\n",
      "AUC Score (Train): 0.908099\n"
     ]
    }
   ],
   "source": [
    "gbm1 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_leaf =60, \n",
    "               min_samples_split =1200, max_features='sqrt', subsample=0.8, random_state=10)\n",
    "gbm1.fit(X,y)\n",
    "y_pred = gbm1.predict(X)\n",
    "y_predprob = gbm1.predict_proba(X)[:,1]\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))\n",
    "# 输出如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比我们最开始完全不调参的拟合效果，可见精确度稍有下降，主要原理是我们使用了0.8的子采样，20%的数据没有参与拟合。\n",
    "\n",
    "现在我们再对最大特征数max_features进行网格搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.82220, std: 0.02250, params: {'max_features': 7},\n",
       "  mean: 0.82241, std: 0.02421, params: {'max_features': 9},\n",
       "  mean: 0.82108, std: 0.02302, params: {'max_features': 11},\n",
       "  mean: 0.82064, std: 0.01900, params: {'max_features': 13},\n",
       "  mean: 0.82198, std: 0.01514, params: {'max_features': 15},\n",
       "  mean: 0.81355, std: 0.02053, params: {'max_features': 17},\n",
       "  mean: 0.81877, std: 0.01863, params: {'max_features': 19}],\n",
       " {'max_features': 9},\n",
       " 0.822412506351626)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_test4 = {'max_features':np.arange(7,20,2)}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_leaf =60, \n",
    "               min_samples_split =1200, subsample=0.8, random_state=10), \n",
    "                       param_grid = param_test4, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch4.fit(X,y)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " 现在我们再对子采样的比例进行网格搜索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.81828, std: 0.02392, params: {'subsample': 0.6},\n",
       "  mean: 0.82344, std: 0.02708, params: {'subsample': 0.7},\n",
       "  mean: 0.81673, std: 0.02196, params: {'subsample': 0.75},\n",
       "  mean: 0.82241, std: 0.02421, params: {'subsample': 0.8},\n",
       "  mean: 0.82285, std: 0.02446, params: {'subsample': 0.85},\n",
       "  mean: 0.81738, std: 0.02236, params: {'subsample': 0.9}],\n",
       " {'subsample': 0.7},\n",
       " 0.8234378969766262)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_leaf =60, \n",
    "               min_samples_split =1200, max_features=9, random_state=10), \n",
    "                       param_grid = param_test5, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch5.fit(X,y)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们基本已经得到我们所有调优的参数结果了。这时我们可以减半步长，最大迭代次数加倍来增加我们模型的泛化能力。再次拟合我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.984\n",
      "AUC Score (Train): 0.905324\n"
     ]
    }
   ],
   "source": [
    "gbm2 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=120,max_depth=7, min_samples_leaf =60, \n",
    "               min_samples_split =1200, max_features=9, subsample=0.7, random_state=10)\n",
    "gbm2.fit(X,y)\n",
    "y_pred = gbm2.predict(X)\n",
    "y_predprob = gbm2.predict_proba(X)[:,1]\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))\n",
    "# 输出如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到AUC分数比起之前的版本稍有下降，这个原因是我们为了增加模型泛化能力，为防止过拟合而减半步长，最大迭代次数加倍，同时减小了子采样的比例，从而减少了训练集的拟合程度。\n",
    "\n",
    "下面我们继续将步长缩小5倍，最大迭代次数增加5倍，继续拟合我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.984\n",
      "AUC Score (Train): 0.908581\n"
     ]
    }
   ],
   "source": [
    "gbm3 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=7, min_samples_leaf =60, \n",
    "               min_samples_split =1200, max_features=9, subsample=0.7, random_state=10)\n",
    "gbm3.fit(X,y)\n",
    "y_pred = gbm3.predict(X)\n",
    "y_predprob = gbm3.predict_proba(X)[:,1]\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))\n",
    "# 输出如下，可见减小步长增加迭代次数可以在保证泛化能力的基础上增加一些拟合程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们继续步长缩小一半，最大迭代次数增加2倍，拟合我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.984\n",
      "AUC Score (Train): 0.908232\n"
     ]
    }
   ],
   "source": [
    "gbm4 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=1200,max_depth=7, min_samples_leaf =60, \n",
    "               min_samples_split =1200, max_features=9, subsample=0.7, random_state=10)\n",
    "gbm4.fit(X,y)\n",
    "y_pred = gbm4.predict(X)\n",
    "y_predprob = gbm4.predict_proba(X)[:,1]\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob))\n",
    "\n",
    "# 输出如下，此时由于步长实在太小，导致拟合效果反而变差，也就是说，步长不能设置的过小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
