{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @Author: King\\n    @Date: 2019.05.16\\n    @Purpose: Attention-Based-BiLSTM-relation-extraction\\n    @Introduction:  Attention-Based-BiLSTM-relation-extraction\\n    @Datasets: Chinese relation extration datasets\\n    @Link : \\n    @Reference : https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding = utf8\n",
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.05.16\n",
    "    @Purpose: Attention-Based-BiLSTM-relation-extraction\n",
    "    @Introduction:  Attention-Based-BiLSTM-relation-extraction\n",
    "    @Datasets: Chinese relation extration datasets\n",
    "    @Link : \n",
    "    @Reference : https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification\n",
    "\n",
    "Tensorflow Implementation of Deep Learning Approach for Relation Extraction Challenge(SemEval-2010 Task #8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals) via Attention-based BiLSTM.\n",
    "\n",
    "Original paper [Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification](http://anthology.aclweb.org/P16-2034) \n",
    "\n",
    "![Attention-Based-BiLSTM-relation-extraction](img/Attention-Based-BiLSTM-relation-extraction.png)\n",
    "\n",
    "\n",
    "### Requrements\n",
    "\n",
    "* Python (>=3.5)\n",
    "\n",
    "* TensorFlow (>=r1.0)\n",
    "\n",
    "* scikit-learn (>=0.18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、Settings Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        '''\n",
    "            Data loading params \n",
    "        '''\n",
    "        ## Path of train data\n",
    "        self.train_path = \"SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\"\n",
    "        self.train_path_cn = \"E:/pythonWp/nlp/relation_extraction/Information-Extraction-Chinese_suss/RE_BGRU_2ATT/origin_data/test_char.txt\"\n",
    "        # Path of test data\n",
    "        self.test_path = \"SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\"\n",
    "        # Path of relation2id data\n",
    "        self.relation2id_path = \"E:/pythonWp/nlp/relation_extraction/Information-Extraction-Chinese_suss/RE_BGRU_2ATT/origin_data/relation2id.txt\"\n",
    "        # Max sentence length in data\n",
    "        self.max_sentence_length = 90\n",
    "        # Percentage of the training data to use for validation\n",
    "        self.dev_sample_percentage = 0.1\n",
    "        \n",
    "        '''\n",
    "            Model Hyper-parameters \n",
    "        '''\n",
    "        '''\n",
    "            1、Embeddings\n",
    "        '''\n",
    "        # Path of pre-trained word embeddings \n",
    "        self.embedding_path = \"E:/pythonWp/nlp/relation_extraction/Information-Extraction-Chinese_suss/RE_BGRU_2ATT/origin_data/vec_char.txt\"\n",
    "        # Dimensionality of word embedding (default: 100)\n",
    "        self.embedding_dim = 100\n",
    "        # Dropout keep probability of embedding layer (default: 0.7)\n",
    "        self.emb_dropout_keep_prob = 0.7\n",
    "        \n",
    "        '''\n",
    "            2、AttLSTM\n",
    "        '''\n",
    "        # Dimensionality of RNN hidden (default: 100)\n",
    "        self.hidden_size = 100\n",
    "        # Dropout keep probability of RNN (default: 0.7)\n",
    "        self.rnn_dropout_keep_prob = 0.7\n",
    "        \n",
    "        '''\n",
    "            3、Misc\n",
    "        '''\n",
    "        # Description for model\n",
    "        self.desc = \"\"\n",
    "        # Dropout keep probability of RNN (default: 0.7)\n",
    "        self.dropout_keep_prob = 0.5\n",
    "        # L2 regularization lambda (default: 1e-5)\n",
    "        self.l2_reg_lambda = 1e-5\n",
    "        \n",
    "        '''\n",
    "            4、Training parameters\n",
    "        '''\n",
    "        # Description for model\n",
    "        self.batch_size = 10\n",
    "        # Number of training epochs (Default: 100)\n",
    "        self.num_epochs = 100\n",
    "        # Number of iterations to display training information\n",
    "        self.display_every = 10\n",
    "        # Evaluate model on dev set after this many steps (default: 100)\n",
    "        self.evaluate_every = 100\n",
    "        # Number of checkpoints to store (default: 5)\n",
    "        self.num_checkpoints = 5\n",
    "        # Which learning rate to start with (Default: 1.0)\n",
    "        self.learning_rate = 1.0\n",
    "        # Decay rate for learning rate (Default: 0.9)\n",
    "        self.decay_rate = 0.9\n",
    "        \n",
    "        '''\n",
    "            5、Testing parameters\n",
    "        '''\n",
    "        # Checkpoint directory from training run\n",
    "        self.checkpoint_dir = \"\"\n",
    "        \n",
    "        '''\n",
    "            6、Misc Parameters\n",
    "        '''\n",
    "        # Allow device soft device placement\n",
    "        self.allow_soft_placement = True\n",
    "        # Log placement of ops on devices\n",
    "        self.log_device_placement = False\n",
    "        # Allow gpu memory growth\n",
    "        self.gpu_allow_growth = True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、数据处理模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    工具包 end\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "'''\n",
    "    工具包 begin\n",
    "'''\n",
    "import sys\n",
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "\n",
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "\n",
    "'''\n",
    "    工具包 end\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取 relation2id 文件数据\n",
    "def load_relation2id_file_cn(filename,demo_flag = False):\n",
    "    '''\n",
    "    读取 data 文件数据\n",
    "    :param filename:    String 文件名称包含路径\n",
    "    :param demo_flag:   String True 只读取 1000 样本数据，Fasle 读取全部数据\n",
    "    :return:\n",
    "        relation2id:   dict    relation to id\n",
    "        id2relation:   list    id to relation \n",
    "    '''\n",
    "    contents_num = 0\n",
    "    relation2id = {}\n",
    "    id2relation = []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data_list = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "                relation2id[data_list[0]] = int(data_list[1])\n",
    "                id2relation.append(data_list[0])\n",
    "                contents_num = contents_num + 1\n",
    "                if demo_flag and contents_num == 500:\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    return relation2id,id2relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 训练集 文件数据\n",
    "def load_data_and_labels_cn(path,settings):\n",
    "    relation2id,id2relation=load_relation2id_file_cn(filename=settings.relation2id_path)   \n",
    "    data = []\n",
    "\n",
    "    df = pd.read_csv(path, sep='\\t',names=['e1','e2','r','s'])\n",
    "    print(\"df:{0}\".format(df.iloc[0:2]))\n",
    "    ''' print(\"df:{0}\".format(df.iloc[0:2]))\n",
    "        output:\n",
    "            df:    \n",
    "            e1  e2    r                                                  s\n",
    "            0  李烈钧  王侃  NaN              李烈钧加入同盟会:光绪三十三年（1907年），经张断、王侃介绍加入同盟会。\n",
    "            1   陈尸  孔子  NaN  子服景伯把这件事告诉给孔子，并且说：“季孙氏已经被公伯寮迷惑了，我的力量能够把公伯寮杀了，把...\n",
    "    '''\n",
    "    max_sentence_length = 0\n",
    "    sentence_list = df['s'].tolist()\n",
    "    for i in range(0,len(sentence_list)):\n",
    "        tokens = str(sentence_list[i])\n",
    "        if max_sentence_length < len(tokens):\n",
    "            max_sentence_length = len(tokens)\n",
    "        sentence_list[i] = \" \".join(tokens)\n",
    "    print(\"sentence_list:{0}\".format(sentence_list[0:2]))\n",
    "    print(\"max sentence length = {}\\n\".format(max_sentence_length))\n",
    "    print(\"df:{0}\".format(df.iloc[0:2]))\n",
    "    ''' print(\"sentence_list:{0}\".format(sentence_list[0:2]))\n",
    "        print(\"max sentence length = {}\\n\".format(max_sentence_length))\n",
    "        print(\"df:{0}\".format(df.iloc[0:2]))\n",
    "        output\n",
    "            sentence_list:['李 烈 钧 加 入 同 盟 会 : 光 绪 三 十 三 年 （ 1 9 0 7 年 ） ， 经 张 断 、 王 侃 介 绍 加 入 同 盟 会  。', '子 服 景 伯 把 这 件 事 告 诉 给 孔 子 ， 并 且 说 ： “ 季 孙 氏 已 经 被 公 伯 寮 迷 惑 了 ， 我 的 力 量 能 够 把 公 伯 寮 杀 了 ， 把 他 陈 尸 于 市 。 ”']\n",
    "            max sentence length = 19751\n",
    "\n",
    "            df:    e1  e2    r                                                  s\n",
    "            0  李烈钧  王侃  NaN              李烈钧加入同盟会:光绪三十三年（1907年），经张断、王侃介绍加入同盟会。\n",
    "            1   陈尸  孔子  NaN  子服景伯把这件事告诉给孔子，并且说：“季孙氏已经被公伯寮迷惑了，我的力量能够把公伯寮杀了，把...\n",
    "    '''\n",
    "\n",
    "    df = df.fillna('NA')            # 将省缺值用 ‘NAN’ 代替\n",
    "\n",
    "    df['label'] = [relation2id[str(r)] for r in df['r']]\n",
    "    print(\"df:{0}\".format(df.iloc[0:2]))\n",
    "    ''' print(\"df:{0}\".format(df.iloc[0:2]))\n",
    "        output:\n",
    "            df:    \n",
    "                    e1  e2   r                                                  s  label\n",
    "            0  李烈钧  王侃  NA              李烈钧加入同盟会:光绪三十三年（1907年），经张断、王侃介绍加入同盟会。      0\n",
    "            1   陈尸  孔子  NA  子服景伯把这件事告诉给孔子，并且说：“季孙氏已经被公伯寮迷惑了，我的力量能够把公伯寮杀了，把...      0\n",
    "    '''\n",
    "\n",
    "    # Text Data\n",
    "    x_text = sentence_list\n",
    "    print(\"x_text:{0}\".format(x_text[0:1]))\n",
    "    ''' print(\"x_text:{0}\".format(x_text[0:1]))\n",
    "        sys.exit(0)\n",
    "        output:\n",
    "            x_text:['李 烈 钧 加 入 同 盟 会 : 光 绪 三 十 三 年 （ 1 9 0 7 年 ） ， 经 张 断 、 王 侃 介 绍 加 入 同 盟 会 。', '子 服 景 伯 把 这 件 事 告 诉 给 孔 子 ， 并 且 说 ： “ 季 孙 氏 已 经 被 公 伯 寮 迷 惑 了 ， 我 的 力 量 能 够 把 公 伯 寮 杀 了 ， 把 他 陈 尸 于 市 。 ”', '剪 辑 ： 朱 小 勤 、 苏 鸿 文', '区 域 创 新 体 系 的 若 干 文 献 综 述 （ 陈 广 胜 许 小 忠 徐 燕 椿 ）', '在 拍 摄 间 隙 的 时 候 ， 谭 松 韵 与 郭 俊 辰 经 常 一 起 吃 辣 条 。']\n",
    "    '''\n",
    "    # Label Data\n",
    "    y = df['label']\n",
    "    labels_flat = y.values.ravel()\n",
    "    labels_count = np.unique(labels_flat).shape[0]\n",
    "    print(\"labels_flat:{0}\".format(labels_flat))\n",
    "    print(\"labels_count:{0}\".format(labels_count))\n",
    "    ''' print(\"labels_flat:{0}\".format(labels_flat))\n",
    "        print(\"labels_count:{0}\".format(labels_count))\n",
    "        output:\n",
    "            labels_flat:[ 0  0  0 ...  0 29 29]\n",
    "            labels_count:35\n",
    "    '''\n",
    "\n",
    "    # convert class labels from scalars to one-hot vectors\n",
    "    # 0  => [1 0 0 0 0 ... 0 0 0 0 0]\n",
    "    # 1  => [0 1 0 0 0 ... 0 0 0 0 0]\n",
    "    # ...\n",
    "    # 18 => [0 0 0 0 0 ... 0 0 0 0 1]\n",
    "    def dense_to_one_hot(labels_dense, num_classes):\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "        return labels_one_hot\n",
    "\n",
    "    labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "    labels = labels.astype(np.uint8)\n",
    "    print(\"x_text:{0}\".format(x_text[0:1]))\n",
    "    print(\"labels:{0}\".format(labels[0:1]))\n",
    "    print(\"len(x_text):{0}\".format(len(x_text)))\n",
    "    print(\"len(labels):{0}\".format(len(labels)))\n",
    "    ''' print(\"x_text:{0}\".format(x_text[0:1]))\n",
    "        print(\"labels:{0}\".format(labels[0:1]))\n",
    "        print(\"len(x_text):{0}\".format(len(x_text)))\n",
    "        print(\"len(labels):{0}\".format(len(labels)))\n",
    "        sys.exit(0)\n",
    "        output:\n",
    "            x_text:['李 烈 钧 加 入 同 盟 会 : 光 绪 三 十 三 年 （ 1 9 0 7 年 ） ， 经 张 断 、 王 侃 介 绍 加 入 同 盟 会 。', '子 服 景 伯 把 这 件 事 告 诉 给 孔 子 ， 并 且 说 ： “ 季 孙 氏 已 经 被 公 伯 寮 迷 惑 了 ， 我 的 力 量 能 够 把 公 伯 寮 杀 了 ， 把 他 陈 尸 于 市 。 ”', '剪 辑 ： 朱 小 勤 、 苏 鸿 文', '区 域 创 新 体 系 的 若 干 文 献 综 述 （ 陈 广 胜 许 小 忠 徐 燕 椿 ）', '在 拍 摄 间 隙 的 时 候 ， 谭 松 韵 与 郭 俊 辰 经 常 一 起 吃 辣 条 。']\n",
    "            labels:[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "             [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "             [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "             [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "             [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
    "            len(x_text):37637\n",
    "            len(labels):37637\n",
    "    '''\n",
    "    return x_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:    e1  e2    r                                                  s\n",
      "0  李烈钧  王侃  NaN              李烈钧加入同盟会:光绪三十三年（1907年），经张断、王侃介绍加入同盟会。\n",
      "1   陈尸  孔子  NaN  子服景伯把这件事告诉给孔子，并且说：“季孙氏已经被公伯寮迷惑了，我的力量能够把公伯寮杀了，把...\n",
      "sentence_list:['李 烈 钧 加 入 同 盟 会 : 光 绪 三 十 三 年 （ 1 9 0 7 年 ） ， 经 张 断 、 王 侃 介 绍 加 入 同 盟 会 。', '子 服 景 伯 把 这 件 事 告 诉 给 孔 子 ， 并 且 说 ： “ 季 孙 氏 已 经 被 公 伯 寮 迷 惑 了 ， 我 的 力 量 能 够 把 公 伯 寮 杀 了 ， 把 他 陈 尸 于 市 。 ”']\n",
      "max sentence length = 19751\n",
      "\n",
      "df:    e1  e2    r                                                  s\n",
      "0  李烈钧  王侃  NaN              李烈钧加入同盟会:光绪三十三年（1907年），经张断、王侃介绍加入同盟会。\n",
      "1   陈尸  孔子  NaN  子服景伯把这件事告诉给孔子，并且说：“季孙氏已经被公伯寮迷惑了，我的力量能够把公伯寮杀了，把...\n",
      "df:    e1  e2   r                                                  s  label\n",
      "0  李烈钧  王侃  NA              李烈钧加入同盟会:光绪三十三年（1907年），经张断、王侃介绍加入同盟会。      0\n",
      "1   陈尸  孔子  NA  子服景伯把这件事告诉给孔子，并且说：“季孙氏已经被公伯寮迷惑了，我的力量能够把公伯寮杀了，把...      0\n",
      "x_text:['李 烈 钧 加 入 同 盟 会 : 光 绪 三 十 三 年 （ 1 9 0 7 年 ） ， 经 张 断 、 王 侃 介 绍 加 入 同 盟 会 。']\n",
      "labels_flat:[ 0  0  0 ...  0 29 29]\n",
      "labels_count:35\n",
      "x_text:['李 烈 钧 加 入 同 盟 会 : 光 绪 三 十 三 年 （ 1 9 0 7 年 ） ， 经 张 断 、 王 侃 介 绍 加 入 同 盟 会 。']\n",
      "labels:[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "len(x_text):37637\n",
      "len(labels):37637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "settings =Settings()\n",
    "load_data_and_labels_cn(path=settings.train_path_cn,settings=settings)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
