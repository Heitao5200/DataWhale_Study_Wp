#encoding=utf-8
import numpy as np
import jieba
from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import CountVectorizer  
 
if __name__ == "__main__":
    docs_list=[]
    mywordlist=[]
    ###############################
    # 1 读取文件，并进行分词
    ###############################

    stopwords_path = "../resource/stopwords.txt" # 停用词词表

    # 读取文件
    file_object = open('../resource/text1.txt','r', encoding='UTF-8')
    try:
      for line in file_object:
          # 文本分词
          seg_list = jieba.cut(line, cut_all=False)
          liststr="/ ".join(seg_list)

           # 读取停用词文件
          f_stop = open(stopwords_path,'r', encoding='UTF-8')
          try:
            f_stop_text = f_stop.read()
          finally:
            f_stop.close()

          # 停用词清除
          f_stop_seg_list = f_stop_text.split('\n')
          for myword in liststr.split('/'):
            if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:
              mywordlist.append(myword)

          docs_list.append(''.join(mywordlist))      # 存入文档列表中
          mywordlist=[]                  # 存入之后，需要清除mywordlist内容，防止重复
    finally:  
      file_object.close()

    # print(f"docs_list:{docs_list}")

    ###############################
    # 2 句子进行 tf-idf 计算
    ###############################

    docs = np.array(docs_list)
    # print(f"docs:{docs}")
    '''
      output:
      docs_list:[
        '当地 时间 2017 15', '日本 神奈川县 横须贺', 
        ' 东芝 国际 反应堆 报废 研究 开发 机构 IRID 共同开发 机器人 公开 亮相', 
        '这个 30 厘米 直径 13 厘米 水下 机器人 投放 福岛 第一 核电站 机组 反应堆 安全壳 底部 展开 调查'
      ]
    docs:[
      '当地 时间 2017 15' '日本 神奈川县 横须贺'
      ' 东芝 国际 反应堆 报废 研究 开发 机构 IRID 共同开发 机器人 公开 亮相'
      '这个 30 厘米 直径 13 厘米 水下 机器人 投放 福岛 第一 核电站 机组 反应堆 安全壳 底部 展开 调查'
    ]
  '''
    # 在scikit-learn中，有两种方法进行TF-IDF的预处理
    # 第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。
    # 第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。

    # 首先我们来看第一种方法，CountVectorizer+TfidfTransformer的组合，代码如下：
    print("-------------第一种方法-------------")
    count = CountVectorizer()
    transformer = TfidfTransformer()
    tfidf = transformer.fit_transform(count.fit_transform(docs)) 
    word=count.get_feature_names()#获取词袋模型中的所有词语
    weight1=tfidf.toarray()#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重
    # print(weight1)
    # for i in range(len(weight1)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重
    #     print(u"-------这里输出第",i,u"类文本的词语tf-idf权重------")
    #     for j in range(len(word)):
    #         print(word[j],weight1[i][j])

    '''
        output:
              [[0.         0.5        0.5        0.         0.         0.
                  0.         0.         0.         0.         0.         0.
                  0.         0.         0.         0.         0.5        0.
                  0.         0.         0.5        0.         0.         0.
                  0.         0.         0.         0.         0.         0.
                  0.         0.         0.         0.        ]
                 [0.         0.         0.         0.         0.         0.
                  0.         0.         0.         0.         0.         0.
                  0.         0.         0.         0.         0.         0.
                  0.         0.57735027 0.         0.         0.         0.
                  0.         0.57735027 0.         0.         0.         0.57735027
                  0.         0.         0.         0.        ]
                 [0.         0.         0.         0.         0.29823274 0.29823274
                  0.29823274 0.29823274 0.29823274 0.         0.23513012 0.29823274
                  0.         0.         0.         0.29823274 0.         0.
                  0.29823274 0.         0.         0.23513012 0.29823274 0.
                  0.         0.         0.         0.         0.29823274 0.
                  0.         0.         0.         0.        ]
                 [0.22796151 0.         0.         0.22796151 0.         0.
                  0.         0.         0.         0.45592301 0.17972747 0.
                  0.22796151 0.22796151 0.22796151 0.         0.         0.22796151
                  0.         0.         0.         0.17972747 0.         0.22796151
                  0.22796151 0.         0.22796151 0.22796151 0.         0.
                  0.22796151 0.22796151 0.22796151 0.22796151]]
    '''

    # 第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理
    print("-------------第二种方法-------------")
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf2 = TfidfVectorizer()
    re = tfidf2.fit_transform(docs)
    word=count.get_feature_names()#获取词袋模型中的所有词语
    weight2=re.toarray()#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重
    # print(weight2)
    # for i in range(len(weight2)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重
    #     print(u"-------这里输出第",i,u"类文本的词语tf-idf权重------")
    #     for j in range(len(word)):
    #         print(word[j],weight2[i][j])

    '''
        output:
            -------------第二种方法-------------
              [[0.         0.5        0.5        0.         0.         0.
              0.         0.         0.         0.         0.         0.
              0.         0.         0.         0.         0.5        0.
              0.         0.         0.5        0.         0.         0.
              0.         0.         0.         0.         0.         0.
              0.         0.         0.         0.        ]
             [0.         0.         0.         0.         0.         0.
              0.         0.         0.         0.         0.         0.
              0.         0.         0.         0.         0.         0.
              0.         0.57735027 0.         0.         0.         0.
              0.         0.57735027 0.         0.         0.         0.57735027
              0.         0.         0.         0.        ]
             [0.         0.         0.         0.         0.29823274 0.29823274
              0.29823274 0.29823274 0.29823274 0.         0.23513012 0.29823274
              0.         0.         0.         0.29823274 0.         0.
              0.29823274 0.         0.         0.23513012 0.29823274 0.
              0.         0.         0.         0.         0.29823274 0.
              0.         0.         0.         0.        ]
             [0.22796151 0.         0.         0.22796151 0.         0.
              0.         0.         0.         0.45592301 0.17972747 0.
              0.22796151 0.22796151 0.22796151 0.         0.         0.22796151
              0.         0.         0.         0.17972747 0.         0.22796151
              0.22796151 0.         0.22796151 0.22796151 0.         0.
              0.22796151 0.22796151 0.22796151 0.22796151]]

    '''
    '''
        output:
            -------这里输出第 0 类文本的词语tf-idf权重------
            13 0.0
            15 0.5
            2017 0.5
            30 0.0
            irid 0.0
            东芝 0.0
            亮相 0.0
            公开 0.0
            共同开发 0.0
            厘米 0.0
            反应堆 0.0
            国际 0.0
            安全壳 0.0
            展开 0.0
            底部 0.0
            开发 0.0
            当地 0.5
            投放 0.0
            报废 0.0
            日本 0.0
            时间 0.5
            机器人 0.0
            机构 0.0
            机组 0.0
            核电站 0.0
            横须贺 0.0
            水下 0.0
            直径 0.0
            研究 0.0
            神奈川县 0.0
            福岛 0.0
            第一 0.0
            调查 0.0
            这个 0.0
            -------这里输出第 1 类文本的词语tf-idf权重------
            13 0.0
            15 0.0
            2017 0.0
            30 0.0
            irid 0.0
            东芝 0.0
            亮相 0.0
            公开 0.0
            共同开发 0.0
            厘米 0.0
            反应堆 0.0
            国际 0.0
            安全壳 0.0
            展开 0.0
            底部 0.0
            开发 0.0
            当地 0.0
            投放 0.0
            报废 0.0
            日本 0.5773502691896257
            时间 0.0
            机器人 0.0
            机构 0.0
            机组 0.0
            核电站 0.0
            横须贺 0.5773502691896257
            水下 0.0
            直径 0.0
            研究 0.0
            神奈川县 0.5773502691896257
            福岛 0.0
            第一 0.0
            调查 0.0
            这个 0.0
            -------这里输出第 2 类文本的词语tf-idf权重------
            13 0.0
            15 0.0
            2017 0.0
            30 0.0
            irid 0.2982327375202219
            东芝 0.2982327375202219
            亮相 0.2982327375202219
            公开 0.2982327375202219
            共同开发 0.2982327375202219
            厘米 0.0
            反应堆 0.2351301157996824
            国际 0.2982327375202219
            安全壳 0.0
            展开 0.0
            底部 0.0
            开发 0.2982327375202219
            当地 0.0
            投放 0.0
            报废 0.2982327375202219
            日本 0.0
            时间 0.0
            机器人 0.2351301157996824
            机构 0.2982327375202219
            机组 0.0
            核电站 0.0
            横须贺 0.0
            水下 0.0
            直径 0.0
            研究 0.2982327375202219
            神奈川县 0.0
            福岛 0.0
            第一 0.0
            调查 0.0
            这个 0.0
            -------这里输出第 3 类文本的词语tf-idf权重------
            13 0.22796150660778644
            15 0.0
            2017 0.0
            30 0.22796150660778644
            irid 0.0
            东芝 0.0
            亮相 0.0
            公开 0.0
            共同开发 0.0
            厘米 0.4559230132155729
            反应堆 0.17972747020412025
            国际 0.0
            安全壳 0.22796150660778644
            展开 0.22796150660778644
            底部 0.22796150660778644
            开发 0.0
            当地 0.0
            投放 0.22796150660778644
            报废 0.0
            日本 0.0
            时间 0.0
            机器人 0.17972747020412025
            机构 0.0
            机组 0.22796150660778644
            核电站 0.22796150660778644
            横须贺 0.0
            水下 0.22796150660778644
            直径 0.22796150660778644
            研究 0.0
            神奈川县 0.0
            福岛 0.22796150660778644
            第一 0.22796150660778644
            调查 0.22796150660778644
            这个 0.22796150660778644
    '''

###############################
# 3 计算句子间的互信息
###############################

from sklearn import metrics as mr
# mr_1to2 = mr.mutual_info_score(weight2[0],weight2[0])
# print(f"mr_1to2:{mr_1to2}")
mr_dict = {}

'''
    print(f"size:{weight1.size}")
    print(f"shape:{weight1.shape}")
    print(f"shape:{weight1.shape[0]}")
    print(f"shape:{weight1.shape[1]}")
    output:
        size:136
        size:(4, 34)
'''
for i in range(weight1.shape[0]):
    for j in range(weight1.shape[0]):
        mr_dict[(i,j)] = mr.mutual_info_score(weight2[i],weight2[j])

for k,v in mr_dict.items():
    print(f"{k}:{v}")

'''
    output:
        (0, 0):0.36221055713544903
        (0, 1):0.011597895004770431
        (0, 2):0.05541453103920141
        (0, 3):0.089413270150857
        (1, 0):0.011597895004770431
        (1, 1):0.29843581270310676
        (1, 2):0.040707680240863825
        (1, 3):0.06543618422258514
        (2, 0):0.05541453103920141
        (2, 1):0.040707680240863825
        (2, 2):0.8082699580001821
        (2, 3):0.4695227557758286
        (3, 0):0.089413270150857
        (3, 1):0.06543618422258514
        (3, 2):0.4695227557758286
        (3, 3):0.9823098238900091
'''
'''
  参考网址：
    Python读取文件的方法：https://www.jianshu.com/p/d8168034917c
    jieba：https://github.com/fxsjy/jieba
        TF-IDF 创建方法：https://www.cnblogs.com/pinard/p/6693230.html
        sklearn：点互信息和互信息：https://blog.csdn.net/u013710265/article/details/72848755
'''